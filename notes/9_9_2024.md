# Lecture 1. 9-9-2024

## Tokenization

- Most statistical NLP applications were trained on labeled natural text
- Tokenization = splitting of text into "words"
    - wordform = specific form of a word (pluralized noun, conjugated verb, etc)
    - take text and split it into a sequence of instances of wordforms
    - punctuation is kinda a big problem
        - like what do you do with contractions, quotes, certain periods, etc
        - should these be seperate tokens?
        - stripping punctuation is generally ok if you don't care about sentences
    - want to apply postprocessing to word forms
        - convert all to lowercase? case sensitive?
        - some may perform lemmatization or stemming

#### lemmatization
    - every word has a base form (dictionary form) called a lemma
        - ex. sing, sang, sung, singing, all share the same lemma
        - words that share same lemma must have the same major part of speech
        - part of speech is not important as it used to be
    - could be done using morpholofical parsing of wordforms
    - more common to use a resource to look up a lemma (WordNet)

#### stemming
    - simpler than lemmatization
    - involves a sequence of rules to convert a word form to a simpler form
    - porter stemmer was a popular method which looked like the following
    ``` 
    ATIONAL -> ATE (eg. relational -> relate)
    SSES -> SS (eg. grasses -> grass)
    ```
    - improved most results, sometimes it made them worse. Sometimes worked, sometimes didn't work

#### sentence segmentation
    - split document into sentences 
    - might seem like you want to do this before tokenizing but more often tokenizing aids sentence segmentation
    - one complication is that end-of-sentence markers are also used for other purposes
    - but these words could ALSO indicate the end of a sentence too

#### chomsky heirarchy (not in textbook)
    - defines four types of formal grammars that are useful for tasks
        1. unrestricted grammars (type 0)
        2. context-sensitive grammars (type 1)
        3. context-free grammars (type 2)
        4. regular grammars (type 3)
    - numbered from least restrivtive (most powerful) to most restrictive (least powerful)
    - often useful to use the most restrictive types of grammar that suits our purpose
    - regular grammars generally powerful enough to specify rules for allowable tokens
    - regular expressions = a grammatical formalism used for defining regular grammars
        - each is a formula in a special language that specifies simple classes of strings (a string = sequence of symbols)
        - there are special symbols that are "operators" that have a special meaning
        - these symbols also have a higharchy of precedence
            1. Parenthesis: ()
            2. Counters: * + ? {} (star is like repeat 0 or more times, + is repeat multiple times, more rules for others)
            3. Sequences and anchors (ex. the, ^my, end$ etc.)
            4. Disjunction: |
        - most modern languages provide libraries allowing users to define and use regular expressions effectively
        - also useful for the lexical analyzer of a compiler
    - useful for defining syntax rules fo a natural language thus for implementing a parser

#### morphology
    - the study of the way that words are built up from smaller meaning-bearing units called morphemes
    - morphological parsing = takes a word form and splits it into its component morphemes
