## simple RNN

- there are no cycles in feedforward neural networks; usually organized into layers
- a recurrent neural network (RNN) = any network that contains a cycle within it's network connections
    - simple RNN has a single hidden layer with outputs that lead back to its own inputs
    - can think of each layer of nodes as a vector; weights as a matrix
    - not simply sending every nodes output to its input
    - output of one node also goes to input at every one of the hidden layer nodes
    - recurrent link could be thought of as a jxj matrix where j = # of hidden layer nodes
    - each recurrent link has its own weight

    - general idea = input one word embedding at a time
        - the hidden layer is kinda like the "updated context" that accounts for everything so far
        - given the context so far and the new word coming in, how do we update the context

    - unrolled RNN = way of displaying RNN at different timesteps
        - usually draw the U matrix sideways
        - helps depict how NN is being executed

    - during forward propagation the weight matrices aren't being updated
        - during training they're being used and updated
        - during inferenced they're fixed 

    - can train a simple RNN with stochastic gradient descent (SGD) and backpropagation
        - but doing multiple updates: "backpropagation through time"
        - when you compute loss at time=2, there's gonna be a delta as to how it's updated and backpropagate the error

    - sequence labeling = any task that involves categorizing every item in a sequence
        - ex. part-of-speech tagging

    - vanishing gradient problem = you do a lot of multiplications with gradients
        - they're initialized to small random values and the shapes of the activation functions (small slopes)
        - when you're backpropagating, a lot of them are going to be zero
        - the further back you go, the closer to zero you get
        - in a sense, the neural network will pay very little attention to early words and only really consider last
        - ReLUs kinda mitigate it but don't work well with RNNs

    - LSTM = long short-term memory
        - common to depict LSTMs graphically in terms of cells
        - cell = component of the architecture that repeats

