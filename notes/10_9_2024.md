## neural networks

- deep neural network = when there's more than 1 hidden layer 

- backpropagation = compute partial derivative of the loss function w.r.t each weight into hidden layers
    - take loss of all outputs and back propogate error
    - give this node some degree of blame for all the errors here

- once gradients for all weights are calculated, they are multiplied by a learning rate that controls the size of the adjustments
    - modern neural networks have adaptive learning rates

- overfitting is a problem due to the large number of adjustable parameters

- methods of regularization that can help mitigate overfitting
    - add an extra term that is added to the loss function to penalize large weights
    - dropout randomly drops units or weights during training

- computation graph = graphical representations of processes for computing mathematical expressions
    - useful for showing how to calculate gradients
    - if you can set up neural network as a computation graph you can automate backpropagation
        - called backward differentation
    - point is that libraries like PyTorch or TensorFlow the libraries will treat the neural networks as computation graphs
        - if you build neural networks with known layers it will do backpropagation automatically for you

- neural networks in NLP
    - used to be feedforward NN with a single hidden layer
        - input node for every distinct word in vocabulary and value of the input was a word weight
        - often led to overfitting
    - word embeddings = take each word and map it to a 300-dimensional vector

## word2vec

- can view words as vectors 
    - in term-document matrix, can view a word as the document it appears in an its count
    - distributional hypothesis = predicts that words with similar semantic meaning will occur in similar contexts

- word embeddings = vector representation of a word
    - idea is to create a d-dimensional vector, with a fixed d, for each word in a vocabulary
    - embeddings are learned from a corpus using an unsupervised learning approach
    - we'll focus on static word embeddings
        - single word embedding is learned for each word (or token) in the training corpus, not taking context into account  
        - ex. 'house' will always have the same embedding no matter its context
        - problem because words can be different in different contexts
    - gpt uses like contextual word embeddings

- in the conventional NLP method for neural networks, two words would be mapped to entirely different nodes or the exact same
- advantages of word embeddings in neural networks
    - word embeddings allow for much smaller sizes of input to neural networks
        - number of input nodes is related to d, the dimension of the word embeddings
        - input might be one word embedding at a time or a fixed number of word embeddings at a time

    - convolutional neural networks = input consists of all word embeddings from one padded sentence at a time
    - recurrent neural networks = typically one word embedding at a time is used as input and words are traversed in a sequence
        - can have any length of sentece as an input which is nice
    - transformers = input consists of all word embeddings from one padded sentence at a time
        - there is a limited size to an input to a transformer

    - no matter what, similar (but non-identical) words will have similar word embeddings
    - house vs. building will have non-identical but similar embeddings

- language model = model that assigns a probability to a sequence of text
    - N-grams were typically used for this purpose
    - N-gram = sequence of N consecutive tokens (often words but can be subwords, characters, embeddings)
        - common N-grams include 2-grams (bigrams) and 3-grams (trigrams); single is unigrams
    - trigram model predicts P(w(i) = "Union" | w(i-2, i-1) = "The Cooper")
- N-gram model computes estimates of the probabilities of each possible final token of an N-gram given the previous N-1 tokens
- natural language generation (NLG)
    - the stuff that it generates or predicts highly depend on what it was trained on

- neural language models
    - how could we use word embeddings to a language model
    - look at how feedforward neural networks can do it
    - map words to embeddings and then feed them as inputs into a neural network which goes to an output layer and chooses the max probability
    - can train this in an unlabeled and unsupervised learning method
        - do like 4-gram, make the output probility of 4th word 1 and rest 0 and then backpropagate and update weights

