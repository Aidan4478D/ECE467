# Lecture 2. 9-11-2024

#### morphology

- stems = main morpheme of the word
- affixes = add additonal meanings to words (prefixes, suffixes, infixes, circumfixes)
- morphologically complex = words can contain significantly more morphemes that typical in other languages

- combining morphemes
    - four methods of combining morphemes to create new words
    - inflection = combination of a word stem with a grammatical morpheme 
        - usally results in word the the same basic POS as the stem
        - usually filling some syntactic function like agreement
        - ex. pluralize a noun ending in "y" to "ies" -> bunny = bunnies
        - some languages can be used to account for grammatical gender, politeless level, etc.
    - derivation = combination of a word stem with a morpheme usually resulting in word of a different class
        - often with a meaning that is harder to predict
        - ex. "clueless" from "clue" or "happiness" from happy"
    - compounding = combining multiple word stems together
        - ex. "dog" and "house" become "doghouse"
    - cliticization = combination of a word stem with a clitic
        - clitic = morpheme that acts like a word but is reduced to a form 

- listeme = a memorized word unit
    - stems generally must be memorized seperately; part of our mental lexicon
    - Pinker claims that 45,000 memorized is an underestimate as the list didn't include names, numbers, etc.

#### out-of-vocabulary words
- words that have not been seen in a training set
- sometimes can be ignored but doesn't make sense in other applications such as machine translation
- in conventional NLP, convert rare words in a training set to a special token and replace them later with the same token
- there are a multitude of reasons they may exist due to new words, mispellings, borrowed from other languages, etc.

#### subwords
- most NLP systems tokenize based on smaller pieces of words = subwords
- morphological parsing can be expensive and methods differ between languages
- individual characters can be used as tokens which map characters to character embeddings (not word embeddings)
    - could seem like a horrible idea but some systems got really good results
- fasttext = goes from text -> subword embeddings
- language specific rules to split words into subwords that is language specific
- learn a vocabulary of subwords from a training set
    - in some text of english, apply some algorithm to learn what are the useful tokens in this
    - one method is byte-pair encoding (BPE) and an alternative is WordPiece

#### byte-pair encoding (BPE)
- originally developed as a compression algorithm
- adapted for tokenization used to generate subword embeddings
- includes a preprocessing step in which a document is split into words generally 
    - involving whitespace, tokenization, and adds an extra end-of-word symbol to each word
- BPE will learn different subwords for different languages
- algorithm
    - get to choose how big the vocabulary size
        - bigger vocabulary size, common or uncommon words have their own tokens
        - also means it might be more computationally expensive
    - starts with a seperate vocabulary symbol for each character in the text being processed
    - do a fixed number of iterations where each iteration will learn another token
        - take the most common sequential pair that's in the training corpus
        - make that pair a new token
        - replace all the instances of the pair with that new token and add to vocabulary list
    - iterations are in the slides
- now take new doc, split into characters, (assume you've seen before)
    - then geedily apply merges in the order in which they were learned
- what you tend to get is that common words are full tokens and others are tokenized into chunks


#### information retrieval
- task of returning, or retrieving, relevant documents in response to a particular natural language query
- IR can be used to retrieve text, image, video, audio (only looking at text or documents that contain text here)
- collection = set of documents being used to satisfy user requests
    - query searches through particular collection
- term = lexical item that occurs in a collection (word, token or phrase)
- bag of words approach = ignore syntactic information
    - not concerned with word order; these are words in document forget about the order
    - not fine for sentiment analysis

#### vector space models
- in this model of IR, documents and queries are represented as vectors
- each location in the vector = a word in vocabulary learned in the training set
    - value associated with each term = term weight
    - often a function of the term's frequency, usually combined with other factors
- for general ML applications, document vectors often include weights of more general features
    - in conventional NLP, features are words in the vocabulary
- with vectors, you can measure the similarity between vectors
    - using cosine similarity metric
    - dot product / normalized dot product
    - computes the cosine between the two vectors, if they point in the same direction that's the most similar they can be
    - typically you dont want larger documents to be more similar just because they have more words

#### term-document matrices
- also consider entire collection of documents to be represented as a sparse matrix of weights
- is a matrix where every column is a vector space model and each weight is a count for ex.
- or each coulmn represents a document and each row represents a term
    - most term weights would be zero irl
